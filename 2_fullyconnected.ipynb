{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1-rc1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '/home/victor/data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `a[:,None]` equivalente à `a.reshape(-1,1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dimensão dos vetores acima é:  \n",
    "- #### dado=(número de objetos, número de features)  #\n",
    "- #### labels=(número de objetos, número de classes) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 17.322098\n",
      "Training accuracy: 11.2%\n",
      "Validation accuracy: 13.1%\n",
      " \n",
      "Loss at step 100: 2.266381\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 70.5%\n",
      " \n",
      "Loss at step 200: 1.834254\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 72.8%\n",
      " \n",
      "Loss at step 300: 1.601070\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 73.9%\n",
      " \n",
      "Loss at step 400: 1.443264\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 74.1%\n",
      " \n",
      "Loss at step 500: 1.324929\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 74.3%\n",
      " \n",
      "Loss at step 600: 1.231132\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 74.6%\n",
      " \n",
      "Loss at step 700: 1.154312\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 74.8%\n",
      " \n",
      "Loss at step 800: 1.089887\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 75.0%\n",
      " \n",
      "Test accuracy: 82.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "      predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      print(' ')\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_subset = 50000\n",
    "Initialized\n",
    "Loss at step 0: 15.193044\n",
    "Training accuracy: 8.2%\n",
    "Validation accuracy: 13.7%\n",
    "Loss at step 100: 1.129415\n",
    "Training accuracy: 86.1%\n",
    "Validation accuracy: 27.8%\n",
    "Loss at step 200: 0.899339\n",
    "Training accuracy: 87.6%\n",
    "Validation accuracy: 28.0%\n",
    "Loss at step 300: 0.773293\n",
    "Training accuracy: 88.2%\n",
    "Validation accuracy: 28.1%\n",
    "Loss at step 400: 0.687697\n",
    "Training accuracy: 88.6%\n",
    "Validation accuracy: 28.0%\n",
    "Loss at step 500: 0.623733\n",
    "Training accuracy: 88.8%\n",
    "Validation accuracy: 27.9%\n",
    "Loss at step 600: 0.573386\n",
    "Training accuracy: 89.1%\n",
    "Validation accuracy: 27.8%\n",
    "Loss at step 700: 0.532616\n",
    "Training accuracy: 89.3%\n",
    "Validation accuracy: 27.6%\n",
    "Loss at step 800: 0.498989\n",
    "Training accuracy: 89.5%\n",
    "Validation accuracy: 27.6%\n",
    "Test accuracy: 28.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 13.791849\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 13.4%\n",
      " \n",
      "Minibatch loss at step 500: 1.590794\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.2%\n",
      " \n",
      "Minibatch loss at step 1000: 1.275118\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.3%\n",
      " \n",
      "Minibatch loss at step 1500: 1.343654\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 77.1%\n",
      " \n",
      "Minibatch loss at step 2000: 0.735087\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.9%\n",
      " \n",
      "Minibatch loss at step 2500: 0.855441\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.5%\n",
      " \n",
      "Minibatch loss at step 3000: 1.344349\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 78.1%\n",
      " \n",
      "Test accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print(' ')\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Numero de 'hidden nodes' exigido pelo problema. Verificaremos se essa escolha será eficiente, ou seja, se a acurácia\n",
    "# irá aumentar\n",
    "number_nodes = 2048\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # Os dados entram no neuronio que tem number_nodes neuronios internos\n",
    "  weights_nodes_in = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, number_nodes]))\n",
    "  biases_nodes_in = tf.Variable(tf.zeros([number_nodes]))\n",
    "  # Agora, a saida do neuronio é o input para o peso a ser calculado pelo modelo, otimizado via SGD\n",
    "  # Assim a entrada deixa de ser cada imagem e passa a ser a saida do neuronio\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([number_nodes, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits_nodes = tf.nn.relu(tf.matmul(tf_train_dataset, weights_nodes_in) + biases_nodes_in)\n",
    "  # Primeiro é feita o cálculo no 'neuronio' e sua saída é usada no calculo seguinte\n",
    "  logits = tf.matmul(logits_nodes, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  # O calculo para os datasets de validação e teste devem ser feitos de maneira idêntica ao treinamento, ou seja,\n",
    "  # deve-se levar em conta que primeiro o dataset passa pela rede neural (1 neuronio nesse caso) e seu output segue para\n",
    "  # o calculo do SGD\n",
    "  valid_nodes = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_nodes_in) + biases_nodes_in)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(valid_nodes, weights) + biases)\n",
    "  test_nodes = tf.nn.relu(tf.matmul(tf_test_dataset, weights_nodes_in) + biases_nodes_in)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_nodes, weights) + biases)\n",
    "# Para facilitar, o nome final das variaveis (resultados) foi mantido inalterado em relação ao exemplo anterior, assim\n",
    "# fica mais rapido fazer as alterações para resolver o que o problema pede e tentar ajustes para melhorar o resultado \n",
    "# final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TESTE PARA REDE NEURAL COM 1 NEURÔNIO COM 1024 NODES INTERNOS-----------------\n",
      "Initialized\n",
      "Minibatch loss at step 0: 498.980011\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 27.1%\n",
      " \n",
      "Minibatch loss at step 500: 25.766361\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      " \n",
      "Minibatch loss at step 1000: 27.368984\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 73.6%\n",
      " \n",
      "Minibatch loss at step 1500: 41.133644\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.3%\n",
      " \n",
      "Minibatch loss at step 2000: 4.025479\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 82.2%\n",
      " \n",
      "Minibatch loss at step 2500: 4.692187\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.5%\n",
      " \n",
      "Minibatch loss at step 3000: 8.905241\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 83.2%\n",
      " \n",
      "Test accuracy: 89.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "print(\"-----------------TESTE PARA REDE NEURAL COM 1 NEURÔNIO COM 1024 NODES INTERNOS-----------------\")\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print(' ')\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NODES Test accuracy:\n",
    "16     86.5 86.5\n",
    "32     85.2 84.3\n",
    "64     87.3 85.1\n",
    "128    86.4 86.2\n",
    "256    87.7 84.6\n",
    "512    87.7 85.8\n",
    "1024   86.0 89.3\n",
    "2048   87.9 89.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes_size = 2**np.arange(4,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados em uma rodada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_final = np.array([86.5, 85.2, 87.3, 86.4, 87.7, 86.3, 86.0, 87.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f99740408d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhpmZleKEYWZmpfx/Y47EwukC5V8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f996c06dac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(nodes_size,test_accuracy_final, '*r')\n",
    "plt.xlabel('Número de nós')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.title('Acurácia x número de nós', fontweight='bold', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados em segunda rodada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy_final2 = np.array([86.5, 84.3, 85.1, 86.2, 84.6, 85.8, 89.3, 89.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f996c047b38>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9974b5aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(nodes_size,test_accuracy_final2, '*r')\n",
    "plt.xlabel('Número de nós')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.title('Acurácia x número de nós - Rodada 2', fontweight='bold', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apesar das variações observadas, para o caso específico que o problema pede (1024 nós) a acurácia obtida foi de 89.3% (86 % ao rodar pela primeira vez) enquanto que para o SGD sem rede neuronal obete-ve 85.7%. Ou seja, não obteve-se um aumento significativo da acurácia (menos de 4%). \n",
    "\n",
    "### Observa-se que para 2048 nós chegou-se em resultado melhor (em valor absoluto), em ambas as rodadas (execução do código). Porém para 1024 nós chega-se em valor de acurácia próximo.\n",
    "\n",
    "### Para um resultado melhor ainda provavelmente só será possível usando mais camadas (neurônios) e também técnicas de regularização (tema do próximo assignment do curso do Udacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
