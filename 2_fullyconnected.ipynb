{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1-rc1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '/home/victor/data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `a[:,None]` equivalente à `a.reshape(-1,1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dimensão dos vetores acima é:  \n",
    "- #### dado=(número de objetos, número de features)  #\n",
    "- #### labels=(número de objetos, número de classes) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 17.322098\n",
      "Training accuracy: 11.2%\n",
      "Validation accuracy: 13.1%\n",
      " \n",
      "Loss at step 100: 2.266381\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 70.5%\n",
      " \n",
      "Loss at step 200: 1.834254\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 72.8%\n",
      " \n",
      "Loss at step 300: 1.601070\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 73.9%\n",
      " \n",
      "Loss at step 400: 1.443264\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 74.1%\n",
      " \n",
      "Loss at step 500: 1.324929\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 74.3%\n",
      " \n",
      "Loss at step 600: 1.231132\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 74.6%\n",
      " \n",
      "Loss at step 700: 1.154312\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 74.8%\n",
      " \n",
      "Loss at step 800: 1.089887\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 75.0%\n",
      " \n",
      "Test accuracy: 82.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "      predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      print(' ')\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_subset = 50000\n",
    "Initialized\n",
    "Loss at step 0: 15.193044\n",
    "Training accuracy: 8.2%\n",
    "Validation accuracy: 13.7%\n",
    "Loss at step 100: 1.129415\n",
    "Training accuracy: 86.1%\n",
    "Validation accuracy: 27.8%\n",
    "Loss at step 200: 0.899339\n",
    "Training accuracy: 87.6%\n",
    "Validation accuracy: 28.0%\n",
    "Loss at step 300: 0.773293\n",
    "Training accuracy: 88.2%\n",
    "Validation accuracy: 28.1%\n",
    "Loss at step 400: 0.687697\n",
    "Training accuracy: 88.6%\n",
    "Validation accuracy: 28.0%\n",
    "Loss at step 500: 0.623733\n",
    "Training accuracy: 88.8%\n",
    "Validation accuracy: 27.9%\n",
    "Loss at step 600: 0.573386\n",
    "Training accuracy: 89.1%\n",
    "Validation accuracy: 27.8%\n",
    "Loss at step 700: 0.532616\n",
    "Training accuracy: 89.3%\n",
    "Validation accuracy: 27.6%\n",
    "Loss at step 800: 0.498989\n",
    "Training accuracy: 89.5%\n",
    "Validation accuracy: 27.6%\n",
    "Test accuracy: 28.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 13.791849\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 13.4%\n",
      " \n",
      "Minibatch loss at step 500: 1.590794\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.2%\n",
      " \n",
      "Minibatch loss at step 1000: 1.275118\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.3%\n",
      " \n",
      "Minibatch loss at step 1500: 1.343654\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 77.1%\n",
      " \n",
      "Minibatch loss at step 2000: 0.735087\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.9%\n",
      " \n",
      "Minibatch loss at step 2500: 0.855441\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.5%\n",
      " \n",
      "Minibatch loss at step 3000: 1.344349\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 78.1%\n",
      " \n",
      "Test accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print(' ')\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Numero de 'hidden nodes' exigido pelo problema. Verificaremos se essa escolha será eficiente, ou seja, se a acurácia\n",
    "# irá aumentar\n",
    "number_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # Os dados entram no neuronio que tem number_nodes neuronios internos\n",
    "  weights_nodes_in = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, number_nodes]))\n",
    "  biases_nodes_in = tf.Variable(tf.zeros([number_nodes]))\n",
    "  # Agora, a saida do neuronio é o input para o peso a ser calculado pelo modelo, otimizado via SGD\n",
    "  # Assim a entrada deixa de ser cada imagem e passa a ser a saida do neuronio\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([number_nodes, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits_nodes = tf.nn.relu(tf.matmul(tf_train_dataset, weights_nodes_in) + biases_nodes_in)\n",
    "  # Primeiro é feita o cálculo no 'neuronio' e sua saída é usada no calculo seguinte\n",
    "  logits = tf.matmul(logits_nodes, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  # O calculo para os datasets de validação e teste devem ser feitos de maneira idêntica ao treinamento, ou seja,\n",
    "  # deve-se levar em conta que primeiro o dataset passa pela rede neural (1 neuronio nesse caso) e seu output segue para\n",
    "  # o calculo do SGD\n",
    "  valid_nodes = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_nodes_in) + biases_nodes_in)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(valid_nodes, weights) + biases)\n",
    "  test_nodes = tf.nn.relu(tf.matmul(tf_test_dataset, weights_nodes_in) + biases_nodes_in)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_nodes, weights) + biases)\n",
    "# Para facilitar, o nome final das variaveis (resultados) foi mantido inalterado em relação ao exemplo anterior, assim\n",
    "# fica mais rapido fazer as alterações para resolver o que o problema pede e tentar ajustes para melhorar o resultado \n",
    "# final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TESTE PARA REDE NEURAL COM 1 NEURÔNIO COM 1024 NODES INTERNOS-----------------\n",
      "Initialized\n",
      "Minibatch loss at step 0: 292.622131\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 24.2%\n",
      " \n",
      "Minibatch loss at step 500: 13.617745\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.7%\n",
      " \n",
      "Minibatch loss at step 1000: 17.567371\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 72.4%\n",
      " \n",
      "Minibatch loss at step 1500: 23.762470\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.6%\n",
      " \n",
      "Minibatch loss at step 2000: 4.461239\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.0%\n",
      " \n",
      "Minibatch loss at step 2500: 3.250837\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.3%\n",
      " \n",
      "Minibatch loss at step 3000: 2.736144\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.2%\n",
      " \n",
      "Test accuracy: 89.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "print(\"-----------------TESTE PARA REDE NEURAL COM 1 NEURÔNIO COM 1024 NODES INTERNOS-----------------\")\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print(' ')\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NODES Test accuracy:\n",
    "16 86.5\n",
    "32 85.2\n",
    "64 87.3\n",
    "128 86.4\n",
    "256  87.7\n",
    "512 86.3\n",
    "1024 86.0\n",
    "2048 87.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes_size = 2**np.arange(4,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_final = np.array([86.5, 85.2, 87.3, 86.4, 87.7, 86.3, 86.0, 87.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f99740408d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhpmZleKEYWZmpfx/Y47EwukC5V8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f996c06dac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(nodes_size,test_accuracy_final, '*r')\n",
    "plt.xlabel('Número de nós')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.title('Acurácia x número de nós', fontweight='bold', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apesar das variações observadas, para o caso específico que o problema pede (1024 nós) a acurácia obtida foi de 86 % enquanto que para o SGD sem rede neuronal obete-ve 85.7%. Ou seja, não obteve-se um aumento significativo da acurácia. \n",
    "\n",
    "### É interessantes notar, pelo gráfico acima, que pode-se obter resultados melhores com menos camadas (nós). Observa-se também que para 2048 nós chegou-se em resultado melhor (em valor absoluto), mas com número bem menor de nós pode-se obter uma acurácia da mesma ordem (com 256 nós a acurácia foi de 87.7%).\n",
    "\n",
    "### Para um resultado melhor ainda provavelmente só será possível usando mais camadas (neurônios) e também técnicas de regularização (tema do próximo assignment do curso do Udacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
